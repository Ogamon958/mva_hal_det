{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "import logging\n",
    "resource.setrlimit(resource.RLIMIT_CORE, (0, 0))\n",
    "logging.info(\"Coredump generation has been disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n",
      "GPU 0: NVIDIA RTX A6000\n",
      "GPU 1: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "# Function to Display Available GPUs\n",
    "def print_available_gpus():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "print_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir('/home/code/import_file')\n",
    "os.chdir('/home/code/features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast  # For Mixed Precision Training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# ===== User-configurable parameters =====\n",
    "mode = 'span'  # 'pooling' or 'span'\n",
    "span_decoder = 'crf'  # 'crf' or 'linear'\n",
    "feature_type = 'norm'  # 'raw' or 'norm'\n",
    "features_to_use = \"key_avg\"  # Select features from key_avg, query_entropy, key_entropy, and lookback_ratio.\n",
    "                             # To select multiple features, separate them with commas, e.g., \"key_avg,query_entropy\"\n",
    "\n",
    "layers = \"0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31\"  # Layers to use\n",
    "heads = \"0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31\"  # Heads to use\n",
    "\n",
    "clf_mode = 'transformer'  # 'transformer' or 'lookback_ratio_lr' # Implementation of a linear regression model for lookback_ratio\n",
    "\n",
    "model_type = 'llama'  # 'llama' or 'qwen'\n",
    "dataset_name = 'summary'  # 'qa', 'data2txt', 'summary', 'truthqa'\n",
    "\n",
    "absolute_path = f'/home/code/data/saves'  # Path to feature data\n",
    "train_path = f'{absolute_path}/{model_type}_{dataset_name}_train.pkl'\n",
    "val_path = f'{absolute_path}/{model_type}_{dataset_name}_val.pkl'\n",
    "test_path = f'{absolute_path}/{model_type}_{dataset_name}_test.pkl'\n",
    "\n",
    "if features_to_use == 'key_avg':\n",
    "    features_to_use_name = 'one'\n",
    "\n",
    "elif features_to_use == 'key_avg,query_entropy,key_entropy':\n",
    "    features_to_use_name = 'all'\n",
    "\n",
    "else:\n",
    "    features_to_use_name = 'others'\n",
    "\n",
    "if mode == 'span':\n",
    "    save_name = f'{model_type}_{dataset_name}_{features_to_use_name}'\n",
    "\n",
    "    if span_decoder == 'crf':\n",
    "        save_name += '_crf'\n",
    "    \n",
    "    if clf_mode == 'lookback_ratio_lr':\n",
    "        save_name += '_lookback_ratio_lr'\n",
    "\n",
    "elif mode == 'pooling':\n",
    "    pooling_type = 'mean'  # Choose one from mean, max, cls, attention, or count\n",
    "    save_name = f'{model_type}_{dataset_name}_{features_to_use_name}_{pooling_type}'\n",
    "\n",
    "script_path_optimize = '/home/code/features/optimize.py'  # Training script\n",
    "script_path_evaluate = '/home/code/features/evaluate.py'  # Evaluation script\n",
    "\n",
    "n_epochs = 150\n",
    "n_trials = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6726636962e4b3197c8889c4837b53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 学習スクリプト実行結果 ===\n",
      "Return code: 0\n",
      "Standard Output:\n",
      "----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 16, 'num_layers': 6, 'dropout_prob': 0.48151508470374516, 'lr': 0.00036040810688745304, 'weight_decay': 0.0001271769521844205} -----\n",
      "トップモデルを /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/top_model_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm_model_dim_256_layers_6_heads_16_rank_1.pt に保存しました。\n",
      "評価指標を /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/eval_metrics_model_dim_256_layers_6_heads_16_rank_1.json に保存しました。\n",
      "----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.498347641955638, 'lr': 0.0004035303242822194, 'weight_decay': 0.00010164691556857959} -----\n",
      "----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.47439365866985805, 'lr': 0.00032191371031902724, 'weight_decay': 9.620883997028516e-05} -----\n",
      "----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.432522936716889, 'lr': 0.0002756881112501042, 'weight_decay': 0.0001348355977978855} -----\n",
      "----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.4841977409184365, 'lr': 0.00031946013000409325, 'weight_decay': 0.00027173265193684474} -----\n",
      "再学習後トップ5モデル情報を/home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/retrained_top_models_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm.jsonに保存しました。\n",
      "\n",
      "Standard Error:\n",
      "2025-02-14 00:32:07,947 - ----- 実行パラメータ -----\n",
      "2025-02-14 00:32:07,947 - Mode: span\n",
      "2025-02-14 00:32:07,947 - Dataset Name: qwen_ragtruth_summary\n",
      "2025-02-14 00:32:07,947 - Feature Type: norm\n",
      "2025-02-14 00:32:07,947 - Notebook Name: 20250214_qwen_norm_summary_attention_only\n",
      "2025-02-14 00:32:07,947 - Train Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_train.pkl\n",
      "2025-02-14 00:32:07,947 - Validation Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_val.pkl\n",
      "2025-02-14 00:32:07,947 - Test Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_test.pkl\n",
      "2025-02-14 00:32:07,947 - Number of Epochs: 150\n",
      "2025-02-14 00:32:07,948 - Number of Trials: 200\n",
      "2025-02-14 00:32:07,948 - Batch Size: 32\n",
      "2025-02-14 00:32:07,948 - Requested Features: ['attention_value']\n",
      "2025-02-14 00:32:07,948 - Layers to use: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "2025-02-14 00:32:07,948 - Heads to use: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "2025-02-14 00:32:07,948 - Classification Mode: transformer\n",
      "2025-02-14 00:32:07,948 - Top Models JSON Path: /home/code/data1/training_file/20250126_qwen_norm_data2txt_attetntion_only_norm/top5_transformer_models_qwen_ragtruth_data2txt_20250126_qwen_norm_data2txt_attetntion_only_norm.json\n",
      "2025-02-14 00:32:07,948 - Span Decoder: crf\n",
      "2025-02-14 00:32:07,948 - ---------------------------\n",
      "2025-02-14 00:32:08,023 - 使用デバイス: cuda:0\n",
      "2025-02-14 00:32:08,023 - 利用可能なGPUの数: 2. DataParallelを使用します。\n",
      "2025-02-14 00:32:08,023 - データの読み込みと前処理開始\n",
      "2025-02-14 00:32:08,023 - 前処理済ファイル /home/code/data1/cashe_file/qwen_0125/preprocessed_data_qwen_ragtruth_summary_norm_all_features.pt を検出。読み込みます。\n",
      "2025-02-14 00:32:28,747 - データ読み込み・前処理完了\n",
      "\u001b[32m[I 2025-02-14 00:32:36,808]\u001b[0m A new study created in memory with name: no-name-e4d9f57d-0839-42ce-8e4e-4c43fb227223\u001b[0m\n",
      "2025-02-14 00:32:36,810 - 指定されたJSONファイル /home/code/data1/training_file/20250126_qwen_norm_data2txt_attetntion_only_norm/top5_transformer_models_qwen_ragtruth_data2txt_20250126_qwen_norm_data2txt_attetntion_only_norm.json からパラメータセットを読み込みます。Optunaをスキップします。\n",
      "2025-02-14 00:32:36,811 - ----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 16, 'num_layers': 6, 'dropout_prob': 0.48151508470374516, 'lr': 0.00036040810688745304, 'weight_decay': 0.0001271769521844205} -----\n",
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:402.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
      "2025-02-14 00:35:06,991 - Epoch 1, train_loss: 0.0888, val_loss: 0.0488, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:35:06,996 - 初回スコア: 0.0000 at epoch 1\n",
      "2025-02-14 00:37:20,565 - Epoch 2, train_loss: 0.0494, val_loss: 0.0452, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:37:20,566 - EarlyStoppingカウンター: 1/10\n",
      "2025-02-14 00:39:34,149 - Epoch 3, train_loss: 0.0477, val_loss: 0.0439, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:39:34,149 - EarlyStoppingカウンター: 2/10\n",
      "2025-02-14 00:41:47,196 - Epoch 4, train_loss: 0.0461, val_loss: 0.0424, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:41:47,196 - EarlyStoppingカウンター: 3/10\n",
      "2025-02-14 00:43:59,936 - Epoch 5, train_loss: 0.0443, val_loss: 0.0416, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:43:59,936 - EarlyStoppingカウンター: 4/10\n",
      "2025-02-14 00:46:13,505 - Epoch 6, train_loss: 0.0434, val_loss: 0.0401, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:46:13,505 - EarlyStoppingカウンター: 5/10\n",
      "2025-02-14 00:48:26,713 - Epoch 7, train_loss: 0.0420, val_loss: 0.0386, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:48:26,713 - EarlyStoppingカウンター: 6/10\n",
      "2025-02-14 00:50:39,637 - Epoch 8, train_loss: 0.0409, val_loss: 0.0393, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:50:39,637 - EarlyStoppingカウンター: 7/10\n",
      "2025-02-14 00:52:52,896 - Epoch 9, train_loss: 0.0400, val_loss: 0.0365, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:52:52,896 - EarlyStoppingカウンター: 8/10\n",
      "2025-02-14 00:55:07,059 - Epoch 10, train_loss: 0.0388, val_loss: 0.0399, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:55:07,060 - EarlyStoppingカウンター: 9/10\n",
      "2025-02-14 00:57:20,768 - Epoch 11, train_loss: 0.0378, val_loss: 0.0353, val_f1: 0.0000, val_precision: 0.0000, val_recall: 0.0000\n",
      "2025-02-14 00:57:20,768 - EarlyStoppingカウンター: 10/10\n",
      "2025-02-14 00:57:20,768 - 早期終了トリガー\n",
      "2025-02-14 00:57:20,768 - Epoch 11で早期終了しました。\n",
      "2025-02-14 00:57:20,771 - best_epoch=1 model復元\n",
      "2025-02-14 00:57:48,284 - トップモデルを /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/top_model_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm_model_dim_256_layers_6_heads_16_rank_1.pt に保存しました。\n",
      "2025-02-14 00:57:48,284 - 評価指標を /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/eval_metrics_model_dim_256_layers_6_heads_16_rank_1.json に保存しました。\n",
      "2025-02-14 00:57:50,155 - ----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.498347641955638, 'lr': 0.0004035303242822194, 'weight_decay': 0.00010164691556857959} -----\n",
      "2025-02-14 00:57:56,956 - OOMエラー => スキップ (continue)\n",
      "2025-02-14 00:57:58,057 - ----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.47439365866985805, 'lr': 0.00032191371031902724, 'weight_decay': 9.620883997028516e-05} -----\n",
      "2025-02-14 00:58:03,713 - OOMエラー => スキップ (continue)\n",
      "2025-02-14 00:58:04,596 - ----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.432522936716889, 'lr': 0.0002756881112501042, 'weight_decay': 0.0001348355977978855} -----\n",
      "2025-02-14 00:58:10,345 - OOMエラー => スキップ (continue)\n",
      "2025-02-14 00:58:11,379 - ----- トップモデルの学習開始: params={'model_dim': 256, 'num_heads': 32, 'num_layers': 6, 'dropout_prob': 0.4841977409184365, 'lr': 0.00031946013000409325, 'weight_decay': 0.00027173265193684474} -----\n",
      "2025-02-14 00:58:17,089 - OOMエラー => スキップ (continue)\n",
      "2025-02-14 00:58:18,204 - 再学習後トップ5モデル情報を/home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/retrained_top_models_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm.jsonに保存しました。\n",
      "2025-02-14 00:58:18,204 - プロセス終了\n",
      "\n",
      "=== 評価スクリプト実行結果 ===\n",
      "Return code: 0\n",
      "Standard Output:\n",
      "Top5 trials file not found. Using retrained file: /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/retrained_top_models_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm.json.\n",
      "Loaded model from /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/top_model_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm_model_dim_256_layers_6_heads_16_rank_1.pt\n",
      "Trial rank_1 - Test F1(span): 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Confusion Matrix:\n",
      "[[120089      0]\n",
      " [  3564      0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99    120089\n",
      "           1       0.00      0.00      0.00      3564\n",
      "\n",
      "    accuracy                           0.97    123653\n",
      "   macro avg       0.49      0.50      0.49    123653\n",
      "weighted avg       0.94      0.97      0.96    123653\n",
      "\n",
      "Test results saved to /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/test_results_trial_rank_1.csv\n",
      "プロセス終了\n",
      "\n",
      "Standard Error:\n",
      "2025-02-14 00:58:22,647 - INFO - ----- 実行パラメータ -----\n",
      "2025-02-14 00:58:22,648 - INFO - Mode: span\n",
      "2025-02-14 00:58:22,648 - INFO - Dataset Name: qwen_ragtruth_summary\n",
      "2025-02-14 00:58:22,648 - INFO - Feature Type: norm\n",
      "2025-02-14 00:58:22,648 - INFO - Notebook Name: 20250214_qwen_norm_summary_attention_only\n",
      "2025-02-14 00:58:22,648 - INFO - Train Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_train.pkl\n",
      "2025-02-14 00:58:22,648 - INFO - Validation Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_val.pkl\n",
      "2025-02-14 00:58:22,648 - INFO - Test Path: /home/code/data1/pkl/qwen_0125/0125_qwen_ragtruth_summary_test.pkl\n",
      "2025-02-14 00:58:22,648 - INFO - Batch Size: 32\n",
      "2025-02-14 00:58:22,648 - INFO - Requested Features: ['attention_value']\n",
      "2025-02-14 00:58:22,648 - INFO - Layers to use: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "2025-02-14 00:58:22,648 - INFO - Heads to use: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "2025-02-14 00:58:22,648 - INFO - Classification Mode: transformer\n",
      "2025-02-14 00:58:22,648 - INFO - Span Decoder: crf\n",
      "2025-02-14 00:58:22,648 - INFO - Top Models JSON Path: None\n",
      "2025-02-14 00:58:22,648 - INFO - Models Directory: /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models\n",
      "2025-02-14 00:58:22,648 - INFO - Output Directory: /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm\n",
      "2025-02-14 00:58:22,648 - INFO - Optuna Trials: 100\n",
      "2025-02-14 00:58:22,648 - INFO - Transformer Epochs: 50\n",
      "2025-02-14 00:58:22,648 - INFO - ---------------------------\n",
      "2025-02-14 00:58:22,706 - INFO - 使用デバイス: cuda:0\n",
      "2025-02-14 00:58:22,707 - INFO - GPUの数: 2\n",
      "2025-02-14 00:58:41,979 - INFO - 前処理済みデータの読み込み完了\n",
      "2025-02-14 00:58:46,104 - WARNING - Top5 trials file not found. Using retrained file: /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/retrained_top_models_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm.json.\n",
      "2025-02-14 00:58:48,448 - INFO - Loaded model from /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/models/top_model_qwen_ragtruth_summary_20250214_qwen_norm_summary_attention_only_norm_model_dim_256_layers_6_heads_16_rank_1.pt\n",
      "2025-02-14 00:59:17,264 - INFO - Trial rank_1 - Test F1(span): 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "2025-02-14 00:59:17,265 - INFO - Confusion Matrix:\n",
      "[[120089      0]\n",
      " [  3564      0]]\n",
      "2025-02-14 00:59:17,265 - INFO - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99    120089\n",
      "           1       0.00      0.00      0.00      3564\n",
      "\n",
      "    accuracy                           0.97    123653\n",
      "   macro avg       0.49      0.50      0.49    123653\n",
      "weighted avg       0.94      0.97      0.96    123653\n",
      "\n",
      "2025-02-14 00:59:18,639 - INFO - Test results saved to /home/code/data1/training_file/20250214_qwen_norm_summary_attention_only_norm/test_results_trial_rank_1.csv\n",
      "2025-02-14 00:59:18,640 - INFO - プロセス終了\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     command_optimize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--top_models_json_path\u001b[39m\u001b[38;5;124m'\u001b[39m, top_models_json_path]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_optimize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== 学習スクリプト実行結果 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mreturncode)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training script execution command\n",
    "command_optimize = [\n",
    "    'python', script_path_optimize,\n",
    "    '--mode', mode,\n",
    "    '--dataset_name', dataset_name,\n",
    "    '--feature_type', feature_type,\n",
    "    '--save_name', save_name,\n",
    "    '--train_path', train_path,\n",
    "    '--val_path', val_path,\n",
    "    '--test_path', test_path,\n",
    "    '--n_epochs', str(n_epochs),\n",
    "    '--n_trials', str(n_trials),\n",
    "    '--batch_size', str(batch_size),\n",
    "    '--features_to_use', features_to_use,\n",
    "    '--layers_to_use', layers,\n",
    "    '--heads_to_use', heads,\n",
    "    '--clf_mode', clf_mode,\n",
    "    '--span_decoder', span_decoder,\n",
    "    '--model_type', model_type\n",
    "]\n",
    "\n",
    "if mode == 'pooling':  # Specify only when using pooling mode\n",
    "    command_optimize += ['--pooling_type', pooling_type]\n",
    "\n",
    "# `top_models_json_path` is used when applying parameters from another model.\n",
    "# If not used, comment it out.\n",
    "#top_models_json_path = '/home/code/data/training_file/llama_data2txt_all_norm/top5_transformer_models_llama_data2txt_all_norm.json'\n",
    "#if top_models_json_path:\n",
    "    #command_optimize += ['--top_models_json_path', top_models_json_path]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command_optimize, capture_output=True, text=True, check=True)\n",
    "    print(\"=== Training Script Execution Result ===\")\n",
    "    print(\"Return code:\", result.returncode)\n",
    "    print(\"Standard Output:\")\n",
    "    print(result.stdout)\n",
    "    print(\"Standard Error:\")\n",
    "    print(result.stderr)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"An error occurred while running the training script.\")\n",
    "    print(\"Return code:\", e.returncode)\n",
    "    print(\"Standard Output:\")\n",
    "    print(e.stdout)\n",
    "    print(\"Standard Error:\")\n",
    "    print(e.stderr)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Training script file not found. Please check the script path.\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred during training.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation script execution command\n",
    "command_evaluate = [\n",
    "    'python', script_path_evaluate,\n",
    "    '--mode', mode,\n",
    "    '--dataset_name', dataset_name,\n",
    "    '--feature_type', feature_type,\n",
    "    '--save_name', save_name,\n",
    "    '--train_path', train_path,\n",
    "    '--val_path', val_path,\n",
    "    '--test_path', test_path,\n",
    "    '--batch_size', str(batch_size),\n",
    "    '--features_to_use', features_to_use,\n",
    "    '--layers_to_use', layers,\n",
    "    '--heads_to_use', heads,\n",
    "    '--clf_mode', clf_mode,\n",
    "    '--span_decoder', span_decoder,\n",
    "    '--model_type', model_type\n",
    "]\n",
    "\n",
    "if mode == 'pooling':  # Specify only when using pooling mode\n",
    "    command_evaluate += ['--pooling_type', pooling_type]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command_evaluate, capture_output=True, text=True, check=True)\n",
    "    print(\"=== Evaluation Script Execution Result ===\")\n",
    "    print(\"Return code:\", result.returncode)\n",
    "    print(\"Standard Output:\")\n",
    "    print(result.stdout)\n",
    "    print(\"Standard Error:\")\n",
    "    print(result.stderr)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"An error occurred while running the evaluation script.\")\n",
    "    print(\"Return code:\", e.returncode)\n",
    "    print(\"Standard Output:\")\n",
    "    print(e.stdout)\n",
    "    print(\"Standard Error:\")\n",
    "    print(e.stderr)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Evaluation script file not found. Please check the script path.\")\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(\"An unexpected error occurred during evaluation.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
