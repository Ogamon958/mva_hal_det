{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Definition\n",
    "task_type= 'QA' #'QA','Data2txt','Summary','truthqa'\n",
    "model_type= 'llama' #'llama','qwen'\n",
    "\n",
    "#qwen's truthqa is not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_type == 'QA':\n",
    "    dataset_name ='qa'\n",
    "elif task_type == 'Data2txt':\n",
    "    dataset_name = 'data2txt'\n",
    "elif task_type == 'Summary':\n",
    "    dataset_name = 'summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import torch\n",
    "import os\n",
    "\n",
    "#GPU Configuration (Specify the GPU to Use)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "#Change Hugging Face Cache (If Necessary)\n",
    "#os.environ['HF_HOME'] = \"/home/code/hallucination/llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 3\n",
      "GPU 0: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 1: NVIDIA RTX 6000 Ada Generation\n",
      "GPU 2: NVIDIA RTX 6000 Ada Generation\n"
     ]
    }
   ],
   "source": [
    "# Function to Display Available GPUs\n",
    "def print_available_gpus():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "print_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource\n",
    "import logging\n",
    "resource.setrlimit(resource.RLIMIT_CORE, (0, 0))\n",
    "logging.info(\"Coredump generation has been disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from gen_features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (task_type == 'QA') or (task_type == 'Data2Txt') or (task_type == 'Summary'):\n",
    "    df = pd.read_json('./data/ragtruth_datasets/rag_truth_merged_data.jsonl', lines=True)\n",
    "    \n",
    "    \n",
    "    #Specify Task\n",
    "    if task_type == 'QA':\n",
    "        df=df[df['task_type'].apply(lambda x: x == 'QA')].reset_index(drop=True)\n",
    "    elif task_type == 'Data2Txt':\n",
    "        df=df[df['task_type'].apply(lambda x: x == 'Data2txt')].reset_index(drop=True)\n",
    "    elif task_type == 'Summary':\n",
    "        df=df[df['task_type'].apply(lambda x: x == 'Summary')].reset_index(drop=True)\n",
    "\n",
    "\n",
    "elif task_type == 'truthqa':\n",
    "    df = pd.read_csv('./data/truthfulqa_datasets/eval_examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>temperature</th>\n",
       "      <th>labels</th>\n",
       "      <th>split</th>\n",
       "      <th>quality</th>\n",
       "      <th>response</th>\n",
       "      <th>task_type</th>\n",
       "      <th>source</th>\n",
       "      <th>source_info</th>\n",
       "      <th>prompt</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14292</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>The phone numbers for several butcher shops ar...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'butcher shop phone number', 'pas...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14292</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Butcher Shop - Hayward phone number: (510) 889...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'butcher shop phone number', 'pas...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14292</td>\n",
       "      <td>mistral-7B-instruct</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[{'start': 102, 'end': 214, 'text': 'However, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, there are several...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'butcher shop phone number', 'pas...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14292</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>0.85</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the provided passages, the butcher sh...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'butcher shop phone number', 'pas...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14292</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, the butcher shop ...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'butcher shop phone number', 'pas...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "      <td>Briefly answer the following question:\\nbutche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>12460</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Eating tomatoes can provide nutrition benefits...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'what are nutrition benefits eati...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>12460</td>\n",
       "      <td>mistral-7B-instruct</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[{'start': 193, 'end': 223, 'text': 'and suppo...</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, it appears that e...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'what are nutrition benefits eati...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>12460</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[{'start': 634, 'end': 690, 'text': 'reducing ...</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the provided passages, here are the n...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'what are nutrition benefits eati...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>12460</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>1.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the provided passages, here are the n...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'what are nutrition benefits eati...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>12460</td>\n",
       "      <td>llama-2-70b-chat</td>\n",
       "      <td>0.85</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>good</td>\n",
       "      <td>Based on the given passages, here are some of ...</td>\n",
       "      <td>QA</td>\n",
       "      <td>MARCO</td>\n",
       "      <td>{'question': 'what are nutrition benefits eati...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "      <td>Briefly answer the following question:\\nwhat a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5934 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_id                model  temperature  \\\n",
       "0         14292           gpt-4-0613         0.70   \n",
       "1         14292   gpt-3.5-turbo-0613         0.70   \n",
       "2         14292  mistral-7B-instruct         0.70   \n",
       "3         14292      llama-2-7b-chat         0.85   \n",
       "4         14292     llama-2-13b-chat         0.70   \n",
       "...         ...                  ...          ...   \n",
       "5929      12460   gpt-3.5-turbo-0613         0.70   \n",
       "5930      12460  mistral-7B-instruct         0.70   \n",
       "5931      12460      llama-2-7b-chat         0.70   \n",
       "5932      12460     llama-2-13b-chat         1.00   \n",
       "5933      12460     llama-2-70b-chat         0.85   \n",
       "\n",
       "                                                 labels  split quality  \\\n",
       "0                                                    []  train    good   \n",
       "1                                                    []  train    good   \n",
       "2     [{'start': 102, 'end': 214, 'text': 'However, ...  train    good   \n",
       "3                                                    []  train    good   \n",
       "4                                                    []  train    good   \n",
       "...                                                 ...    ...     ...   \n",
       "5929                                                 []  train    good   \n",
       "5930  [{'start': 193, 'end': 223, 'text': 'and suppo...  train    good   \n",
       "5931  [{'start': 634, 'end': 690, 'text': 'reducing ...  train    good   \n",
       "5932                                                 []  train    good   \n",
       "5933                                                 []  train    good   \n",
       "\n",
       "                                               response task_type source  \\\n",
       "0     The phone numbers for several butcher shops ar...        QA  MARCO   \n",
       "1     Butcher Shop - Hayward phone number: (510) 889...        QA  MARCO   \n",
       "2     Based on the given passages, there are several...        QA  MARCO   \n",
       "3     Based on the provided passages, the butcher sh...        QA  MARCO   \n",
       "4     Based on the given passages, the butcher shop ...        QA  MARCO   \n",
       "...                                                 ...       ...    ...   \n",
       "5929  Eating tomatoes can provide nutrition benefits...        QA  MARCO   \n",
       "5930  Based on the given passages, it appears that e...        QA  MARCO   \n",
       "5931  Based on the provided passages, here are the n...        QA  MARCO   \n",
       "5932  Based on the provided passages, here are the n...        QA  MARCO   \n",
       "5933  Based on the given passages, here are some of ...        QA  MARCO   \n",
       "\n",
       "                                            source_info  \\\n",
       "0     {'question': 'butcher shop phone number', 'pas...   \n",
       "1     {'question': 'butcher shop phone number', 'pas...   \n",
       "2     {'question': 'butcher shop phone number', 'pas...   \n",
       "3     {'question': 'butcher shop phone number', 'pas...   \n",
       "4     {'question': 'butcher shop phone number', 'pas...   \n",
       "...                                                 ...   \n",
       "5929  {'question': 'what are nutrition benefits eati...   \n",
       "5930  {'question': 'what are nutrition benefits eati...   \n",
       "5931  {'question': 'what are nutrition benefits eati...   \n",
       "5932  {'question': 'what are nutrition benefits eati...   \n",
       "5933  {'question': 'what are nutrition benefits eati...   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     Briefly answer the following question:\\nbutche...   \n",
       "1     Briefly answer the following question:\\nbutche...   \n",
       "2     Briefly answer the following question:\\nbutche...   \n",
       "3     Briefly answer the following question:\\nbutche...   \n",
       "4     Briefly answer the following question:\\nbutche...   \n",
       "...                                                 ...   \n",
       "5929  Briefly answer the following question:\\nwhat a...   \n",
       "5930  Briefly answer the following question:\\nwhat a...   \n",
       "5931  Briefly answer the following question:\\nwhat a...   \n",
       "5932  Briefly answer the following question:\\nwhat a...   \n",
       "5933  Briefly answer the following question:\\nwhat a...   \n",
       "\n",
       "                                          combined_text  \n",
       "0     Briefly answer the following question:\\nbutche...  \n",
       "1     Briefly answer the following question:\\nbutche...  \n",
       "2     Briefly answer the following question:\\nbutche...  \n",
       "3     Briefly answer the following question:\\nbutche...  \n",
       "4     Briefly answer the following question:\\nbutche...  \n",
       "...                                                 ...  \n",
       "5929  Briefly answer the following question:\\nwhat a...  \n",
       "5930  Briefly answer the following question:\\nwhat a...  \n",
       "5931  Briefly answer the following question:\\nwhat a...  \n",
       "5932  Briefly answer the following question:\\nwhat a...  \n",
       "5933  Briefly answer the following question:\\nwhat a...  \n",
       "\n",
       "[5934 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Model\n",
    "if model_type == 'llama':\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    hf_token = \"aaaaa\" #Set Token Manually\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True,torch_dtype=torch.float16,token=hf_token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,token=hf_token)\n",
    "    \n",
    "\n",
    "elif model_type == 'qwen':\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True,torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "\n",
    "matrix_device_v = torch.device(\"cuda:1\")\n",
    "matrix_device_o = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Prompt + Also Serve as a Test\n",
    "if model_type == 'llama':\n",
    "    if task_type != 'truthqa':\n",
    "        system_prompt=\"You are an excellent system, generating output according to the instructions.\"\n",
    "        header_prompt=\"<|start_header_id|>system<|end_header_id|>\"\n",
    "        user_prompt=\"<|eot_id|><|start_header_id|>user<|end_header_id|>\"\n",
    "        assistant_prompt=\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "        \n",
    "        \n",
    "        #test\n",
    "        message_prompt= df.loc[0, 'prompt']\n",
    "        response = df.loc[0, 'response']\n",
    "        full_prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n{response}<|eot_id|>\"\n",
    "        output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio=get_features_no_generate(full_prompt, model, tokenizer, device, matrix_device_v, matrix_device_o)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    else:\n",
    "        system_prompt=\"You are an excellent question-answering system. Respond to the given questions in 30 words or less.\"\n",
    "        header_prompt=\"<|start_header_id|>system<|end_header_id|>\"\n",
    "        user_prompt=\"<|eot_id|><|start_header_id|>user<|end_header_id|>\"\n",
    "        assistant_prompt=\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "        \n",
    "        #test\n",
    "        message_prompt=\"Q: What is the capital of Japan?\\nA:\"\n",
    "        temperature=1.50\n",
    "        top_k=100\n",
    "        top_p=1.5\n",
    "        max_new_tokens=70\n",
    "        seed=42\n",
    "        \n",
    "        prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n\"\n",
    "        output_text,generated_text, output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio=get_features_with_generate(prompt, model, tokenizer, device, matrix_device_v, matrix_device_o,temperature, top_k, top_p, seed, max_new_tokens)\n",
    "        \n",
    "\n",
    "\n",
    "elif model_type=='qwen':\n",
    "    if task_type != 'truthqa':\n",
    "        system_prompt=\"You are an excellent system, generating output according to the instructions.\"\n",
    "        header_prompt=\"<|im_start|>system\"\n",
    "        user_prompt=\"<|im_end|>\\n<|im_start|>user\"\n",
    "        assistant_prompt=\"<|im_end|>\\n<|im_start|>assistant\"\n",
    "        \n",
    "        #test\n",
    "        message_prompt= df.loc[0, 'prompt']\n",
    "        response = df.loc[0, 'response']\n",
    "        full_prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n{response}<|im_end|>\"\n",
    "        \n",
    "        output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio=get_features_no_generate(full_prompt, model, tokenizer, device, matrix_device_v, matrix_device_o)\n",
    "    \n",
    "\n",
    "print(output_tokens)\n",
    "print(raw_key_avg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute (Including Saving)\n",
    "if task_type != 'truthqa':\n",
    "    for i in tqdm(range(len(df))):\n",
    "        message_prompt= df.loc[i, 'prompt']\n",
    "        response = df.loc[i, 'response']\n",
    "\n",
    "        if model_type == 'llama':\n",
    "            full_prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n{response}<|eot_id|>\"\n",
    "        elif model_type == 'qwen':\n",
    "            full_prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n{response}<|im_end|>\"\n",
    "        \n",
    "        output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio=get_features_no_generate(full_prompt, model, tokenizer, device, matrix_device_v, matrix_device_o)            \n",
    "        data.append([full_prompt,output_tokens,raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio])\n",
    "        \n",
    "    result_df = pd.DataFrame(data, columns=['full_prompt','output_tokens','raw_key_avg', 'raw_query_entropy', 'raw_key_entropy','raw_lookback_ratio', 'norm_key_avg', 'norm_query_entropy', 'norm_key_entropy','norm_lookback_ratio'])\n",
    "\n",
    "    #Merge result_df and df\n",
    "    df = pd.concat([df, result_df], axis=1)\n",
    "    \n",
    "    #save\n",
    "    df.to_pickle(f'./data/saves/{model_type}_{dataset_name}.pkl')\n",
    "    \n",
    "\n",
    "else: #In the Case of TruthfulQA\n",
    "    temperature=1.50\n",
    "    top_k=100\n",
    "    top_p=1.5\n",
    "    max_new_tokens=70\n",
    "    seed_values=[2,12,22,32,42,52,62,72,82,92]\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        question = df.loc[i, 'Question']\n",
    "        message_prompt=f\"Q: {question}\\nA:\"\n",
    "        prompt=f\"{header_prompt}\\n{system_prompt}{user_prompt}\\n{message_prompt}{assistant_prompt}\\n\"\n",
    "        \n",
    "        for seed in seed_values:\n",
    "            output_text,generated_text, output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio=get_features_with_generate(prompt, model, tokenizer, device, matrix_device_v, matrix_device_o,temperature, top_k, top_p, seed, max_new_tokens)\n",
    "            data.append([question,prompt,output_text,generated_text, output_tokens, raw_key_avg, raw_query_entropy, raw_key_entropy,raw_lookback_ratio,norm_key_avg, norm_query_entropy, norm_key_entropy,norm_lookback_ratio])\n",
    "    \n",
    "    \n",
    "    # Convert List to DataFrame\n",
    "    result_df = pd.DataFrame(data, columns=['question', 'prompt', 'output_text', 'generated_text', 'output_tokens', 'raw_key_avg', 'raw_query_entropy', 'raw_key_entropy', 'raw_lookback_ratio', 'norm_key_avg', 'norm_query_entropy', 'norm_key_entropy', 'norm_lookback_ratio'])\n",
    "    \n",
    "    #save\n",
    "    result_df.to_pickle(f'./data/saves/{model_type}_{task_type}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
