{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/code/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify model_type\n",
    "model_type= 'llama' #'llama','qwen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RagTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-09 02:54:55,799 INFO: Coredump generation has been disabled.\n",
      "2024-12-09 02:54:55,804 INFO: データを読み込んでいます: /home/code/vishnu/pkl/1206_ragtruth_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Get Execution Timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Configure Execution Log\n",
    "# Create Log Filename\n",
    "log_filename = f'process_preprocessing_{model_type}_{timestamp}.log'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Disable Core Dump Generation (Unix/Linux Only)\n",
    "try:\n",
    "    import resource\n",
    "    resource.setrlimit(resource.RLIMIT_CORE, (0, 0))\n",
    "    logging.info(\"Coredump generation has been disabled.\")\n",
    "except (ImportError, AttributeError):\n",
    "    # Skip If resource Module or RLIMIT_CORE Is Not Available\n",
    "    logging.warning(\"Could not disable coredump generation on this system.\")\n",
    "\n",
    "# Reconstruct Text (Add Spaces Based on Tokens)\n",
    "def reconstruct_text_with_spaces(tokens):\n",
    "    if not tokens:\n",
    "        return ''\n",
    "    result = tokens[0].replace('Ġ', '')\n",
    "    for token in tokens[1:]:\n",
    "        if 'Ġ' in token:\n",
    "            result += ' ' + token.replace('Ġ', '')\n",
    "        else:\n",
    "            result += token\n",
    "    return result\n",
    "\n",
    "# Normalize Text (Remove Punctuation, Special Characters, and Extra Spaces)\n",
    "def normalize_text(text):\n",
    "    text = text.replace('Â', '').replace('°', '°')  # Correct Special Characters\n",
    "    text = re.sub(r'[^\\w\\s\\-]', '', text)  # Retain Hyphens, Remove Other Punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)       # Remove Extra Spaces\n",
    "    return text.strip()                    # Trim Leading and Trailing Spaces\n",
    "\n",
    "# Retrieve Hallucination Data and Token Positions\n",
    "def get_hallucination_data(row):\n",
    "    hallucinations = []\n",
    "    hallucination_texts = []\n",
    "    hallucination_tokens = []\n",
    "\n",
    "    cleaned_tokens = [token.replace('Ġ', '') for token in row['output_tokens']]\n",
    "    combined_text = ''.join(cleaned_tokens)\n",
    "\n",
    "    for label in row['labels']:\n",
    "        hallucination_text = label['text'].replace('\\n', 'Ċ')\n",
    "        search_text = hallucination_text.replace(' ', '')\n",
    "        position = combined_text.find(search_text)\n",
    "\n",
    "        if position != -1:\n",
    "            current_pos = 0\n",
    "            start_token = end_token = None\n",
    "            for idx, token in enumerate(cleaned_tokens):\n",
    "                current_pos += len(token)\n",
    "                if current_pos > position and start_token is None:\n",
    "                    start_token = idx\n",
    "                if current_pos >= position + len(search_text):\n",
    "                    end_token = idx + 1\n",
    "                    break\n",
    "            if start_token is not None and end_token is not None:\n",
    "                hallucinations.append({'start': start_token, 'end': end_token})\n",
    "                hallucination_texts.append(hallucination_text)\n",
    "                hallucination_tokens.append(row['output_tokens'][start_token:end_token])\n",
    "\n",
    "    return hallucinations, hallucination_texts, hallucination_tokens\n",
    "\n",
    "# Generate Hallucination Labels (1: Hallucination Token, 0: Others)\n",
    "def generate_hallucination_labels(output_tokens, hallucinations):\n",
    "    label_list = [0] * len(output_tokens)\n",
    "    for hal in hallucinations:\n",
    "        for i in range(hal['start'], hal['end']):\n",
    "            if 0 <= i < len(label_list):\n",
    "                label_list[i] = 1\n",
    "    return label_list\n",
    "\n",
    "# Create all_texts_hal_token_label Column\n",
    "def generate_all_texts_label(output_tokens, hal_token_label, task_name):\n",
    "    try:\n",
    "        if task_name.lower() == 'data2txt':\n",
    "            search_tokens = ['Overview', ':']\n",
    "        else:\n",
    "            search_tokens = ['output', ':']\n",
    "\n",
    "        search_index = output_tokens.index(search_tokens[0], 0)\n",
    "        # Assuming '+7' refers to skipping 7 tokens after the search_tokens[0]\n",
    "        start_index = search_index + 7  \n",
    "        \n",
    "        if model_type == 'llama':\n",
    "            end_index = output_tokens.index('<|eot_id|>', start_index)\n",
    "        \n",
    "        elif model_type == 'qwen':\n",
    "            end_index = output_tokens.index('<|im_end|>', start_index)\n",
    "        \n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    prefix = [-1] * start_index\n",
    "    hal_token_part = hal_token_label[start_index:end_index]\n",
    "    final_label = -1\n",
    "    return prefix + hal_token_part + [final_label]\n",
    "\n",
    "# Get Token Index of Answer Start Position\n",
    "def generate_answer_start_token_index(output_tokens, task_name):\n",
    "    if task_name.lower() == 'data2txt':\n",
    "        search_tokens = ['Overview', ':']\n",
    "    else:\n",
    "        search_tokens = ['output', ':']\n",
    "\n",
    "    for i in range(len(output_tokens) - 1):\n",
    "        if output_tokens[i] == search_tokens[0] and output_tokens[i + 1] == search_tokens[1]:\n",
    "            return i + 7\n",
    "    return None\n",
    "\n",
    "# Process Data, Reconstruct Text, and Generate Labels\n",
    "def process_data(row, task_name):\n",
    "    tokens = row['output_tokens']\n",
    "    labels = row['labels']\n",
    "\n",
    "    start_index = generate_answer_start_token_index(tokens, task_name)\n",
    "    if start_index is None:\n",
    "        return pd.Series({\n",
    "            'reconstructed_text': None,\n",
    "            'token_to_word_map': None,\n",
    "            'hal_tokens': None,\n",
    "            'hal_texts': None,\n",
    "            'hal_tokens_text': None,\n",
    "            'hal_words': None,\n",
    "            'hal_token_label': None,\n",
    "            'all_texts_hal_token_label': None,\n",
    "            'answer_start_token_index': None,\n",
    "            'answer_start_text': None\n",
    "        })\n",
    "\n",
    "    words = []\n",
    "    token_to_word_map = {}\n",
    "    current_word = \"\"\n",
    "    current_word_index = -1\n",
    "\n",
    "    for idx in range(start_index, len(tokens)):\n",
    "        token = tokens[idx]\n",
    "        if 'Ġ' in token:\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            current_word = token.replace('Ġ', '')\n",
    "            current_word_index += 1\n",
    "        else:\n",
    "            current_word += token\n",
    "\n",
    "        token_to_word_map[idx] = current_word_index\n",
    "\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "\n",
    "    for idx in range(start_index):\n",
    "        token_to_word_map[idx] = -1\n",
    "\n",
    "    reconstructed_text = ' '.join(words)\n",
    "\n",
    "    # Retrieve Hallucination Data\n",
    "    hallucinations = []\n",
    "    hallucination_texts = []\n",
    "    hallucination_tokens = []\n",
    "    hallucinations_word_indices = []\n",
    "\n",
    "    cleaned_tokens = [token.replace('Ġ', '') for token in tokens]\n",
    "    combined_text = ''.join(cleaned_tokens)\n",
    "\n",
    "    for label in labels:\n",
    "        hallucination_text = label['text'].replace('\\n', 'Ċ')\n",
    "        search_text = hallucination_text.replace(' ', '')\n",
    "        position = combined_text.find(search_text)\n",
    "\n",
    "        if position != -1:\n",
    "            current_pos = 0\n",
    "            start_token = end_token = None\n",
    "            for idx, token in enumerate(cleaned_tokens):\n",
    "                current_pos += len(token)\n",
    "                if current_pos > position and start_token is None:\n",
    "                    start_token = idx\n",
    "                if current_pos >= position + len(search_text):\n",
    "                    end_token = idx + 1\n",
    "                    break\n",
    "\n",
    "            if start_token is not None and end_token is not None:\n",
    "                hallucinations.append({'start': start_token, 'end': end_token})\n",
    "                hallucination_texts.append(hallucination_text)\n",
    "                hallucination_tokens.append(tokens[start_token:end_token])\n",
    "\n",
    "                hallucinations_word_indices.append({\n",
    "                    'start': token_to_word_map[start_token],\n",
    "                    'end': token_to_word_map[end_token - 1] + 1\n",
    "                })\n",
    "\n",
    "    # Generate Hallucination Labels\n",
    "    hal_token_label = [0] * len(tokens)\n",
    "    for hal in hallucinations:\n",
    "        for i in range(hal['start'], hal['end']):\n",
    "            if 0 <= i < len(hal_token_label):\n",
    "                hal_token_label[i] = 1\n",
    "\n",
    "    # Generate all_texts_hal_token_label\n",
    "    all_texts_hal_token_label = generate_all_texts_label(tokens, hal_token_label, task_name)\n",
    "\n",
    "    return pd.Series({\n",
    "        'reconstructed_text': reconstructed_text,\n",
    "        'token_to_word_map': token_to_word_map,\n",
    "        'hal_tokens': hallucinations,\n",
    "        'hal_texts': hallucination_texts,\n",
    "        'hal_tokens_text': hallucination_tokens,\n",
    "        'hal_words': hallucinations_word_indices,\n",
    "        'hal_token_label': hal_token_label,\n",
    "        'all_texts_hal_token_label': all_texts_hal_token_label,\n",
    "        'answer_start_token_index': start_index,\n",
    "        'answer_start_text': reconstruct_text_with_spaces(tokens[start_index:]) if start_index is not None else None\n",
    "    })\n",
    "\n",
    "#  Split DataFrame into Training and Validation Sets Based on source_id\n",
    "def split_train_dev_by_source_id(train_dev_df, val_size=75, seed=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training and validation sets based on unique `source_id`.\n",
    "\n",
    "    Parameters:\n",
    "    - train_dev_df (pd.DataFrame): The DataFrame to be split.\n",
    "    - val_size (int): The number of unique `source_id` for the validation set.\n",
    "    - seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_df (pd.DataFrame): The training set DataFrame.\n",
    "    - val_df (pd.DataFrame): The validation set DataFrame.\n",
    "    \"\"\"\n",
    "    # Retrieve Unique source_id\n",
    "    unique_source_ids = train_dev_df['source_id'].unique()\n",
    "\n",
    "    # Fix Random Seed to Reproduce the Same Split\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Randomly Select source_id for Validation Set\n",
    "    val_size = min(val_size, len(unique_source_ids))  # Ensure val_size Does Not Exceed the Number of Unique source_id\n",
    "    dev_source_ids = np.random.choice(unique_source_ids, size=val_size, replace=False)\n",
    "\n",
    "    # Split DataFrame Based on source_id\n",
    "    val_df = train_dev_df[train_dev_df['source_id'].isin(dev_source_ids)]\n",
    "    train_df = train_dev_df[~train_dev_df['source_id'].isin(dev_source_ids)]\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "\n",
    "# Main Processing Function (Supports Multiple Tasks)\n",
    "def main_processing(df, task_name, dataset_type, sentence_dir='/home/code/data/saves'):\n",
    "    logging.info(f\"Retrieving hallucination data from {dataset_type} data for the {task_name} task...\")\n",
    "    # Retrieve Hallucination Data\n",
    "    df[['hal_tokens', 'hal_texts', 'hal_tokens_text']] = df.apply(lambda row: pd.Series(get_hallucination_data(row)), axis=1)\n",
    "    \n",
    "    # Free Memory\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"Generating hallucination labels from {dataset_type} data for the {task_name} task...\")\n",
    "    # Generate hal_token_label\n",
    "    df['hal_token_label'] = df.apply(lambda row: generate_hallucination_labels(row['output_tokens'], row['hal_tokens']), axis=1)\n",
    "    \n",
    "    # Free Memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Generate all_texts_hal_token_label\n",
    "    df['all_texts_hal_token_label'] = df.apply(lambda row: generate_all_texts_label(row['output_tokens'], row['hal_token_label'], task_name), axis=1)\n",
    "    \n",
    "    # Free Memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Filter Rows Where Hallucination Occurs\n",
    "    hallucination_rows = df[df['hal_token_label'].apply(lambda x: 1 in x if x is not None else False)]\n",
    "    \n",
    "    logging.info(f\"Number of rows with hallucination in {dataset_type} data for the {task_name} task: {hallucination_rows.shape[0]}\")\n",
    "\n",
    "    logging.info(f\"Processing {dataset_type} data for the {task_name} task...\")\n",
    "    \n",
    "    # Data Processing\n",
    "    df[['reconstructed_text', 'token_to_word_map', 'hal_tokens', 'hal_texts',\n",
    "        'hal_tokens_text', 'hal_words', 'hal_token_label',\n",
    "        'all_texts_hal_token_label', 'answer_start_token_index',\n",
    "        'answer_start_text']] = df.apply(lambda row: process_data(row, task_name), axis=1)\n",
    "    \n",
    "    # Free Memory\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"Saving hallucination results to a CSV file for {dataset_type} data in the {task_name} task...\")\n",
    "    # Generate hallucination_results.csv and hallucination_results_false.csv\n",
    "    output_data = []\n",
    "    for index, row in hallucination_rows.iterrows():\n",
    "        for hal_text, hal_tokens_text, hal_tokens_indices in zip(row['hal_texts'], row['hal_tokens_text'], row['hal_tokens']):\n",
    "            start = hal_tokens_indices['start']\n",
    "            end = hal_tokens_indices['end']\n",
    "            reconstructed_text = reconstruct_text_with_spaces(row['output_tokens'][start:end])\n",
    "            extracted_tokens_text = reconstructed_text\n",
    "\n",
    "            is_match = normalize_text(hal_text) == normalize_text(reconstructed_text) or normalize_text(hal_text) in normalize_text(reconstructed_text)\n",
    "\n",
    "            output_data.append({\n",
    "                'Row': index,\n",
    "                'Hallucination Tokens (Indices)': f\"{start}-{end}\",\n",
    "                'Hallucination Text': hal_text,\n",
    "                'Extracted Tokens Text': extracted_tokens_text,\n",
    "                'Extracted Hallucination Tokens': row['output_tokens'][start:end],\n",
    "                'Text Match': is_match\n",
    "            })\n",
    "\n",
    "    if output_data:\n",
    "        # Save DataFrame\n",
    "        output_df = pd.DataFrame(output_data)\n",
    "        csv_dir = '/home/code/data/saves/csv'\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "        output_csv_path = f\"{csv_dir}/{model_type}_{task_name}_{dataset_type}_hallucination_results.csv\"\n",
    "        false_csv_path = f\"{csv_dir}/{model_type}_{task_name}_{dataset_type}_hallucination_results_false.csv\"\n",
    "        output_df.to_csv(output_csv_path, index=False)\n",
    "        false_match_df = output_df[output_df['Text Match'] == False]\n",
    "        false_match_df.to_csv(false_csv_path, index=False)\n",
    "        \n",
    "        # Free Memory\n",
    "        del output_data, output_df, false_match_df\n",
    "        gc.collect()\n",
    "\n",
    "        logging.info(f\"Hallucination results for {dataset_type} data in the {task_name} task have been saved to a CSV file.\")\n",
    "        logging.info(f\"All results have been saved to {output_csv_path}, and only False results have been saved to {false_csv_path}.\")\n",
    "    else:\n",
    "        logging.info(f\"No rows containing hallucination were found in {dataset_type} data for the {task_name} task.\")\n",
    "\n",
    "    # Save DataFrame (Pickle Format)\n",
    "    os.makedirs(sentence_dir, exist_ok=True)\n",
    "    if dataset_type == 'train_dev':\n",
    "        train_df, val_df = split_train_dev_by_source_id(df[df['split'] == 'train'], val_size=75, seed=42)\n",
    "        train_pickle_path = f\"{sentence_dir}/{model_type}_{task_name}_train.pkl\"\n",
    "        val_pickle_path = f\"{sentence_dir}/{model_type}_{task_name}_val.pkl\"\n",
    "        train_df.to_pickle(train_pickle_path)\n",
    "        val_df.to_pickle(val_pickle_path)\n",
    "        logging.info(f\"Train and validation data for the {task_name} task have been saved in Pickle format.\")\n",
    "    elif dataset_type == 'test':\n",
    "        test_df = df[df['split'] == 'test']\n",
    "        test_pickle_path = f\"{sentence_dir}/{model_type}_{task_name}_test.pkl\"\n",
    "        test_df.to_pickle(test_pickle_path)\n",
    "        logging.info(f\"Test data for the {task_name} task have been saved in Pickle format.\")\n",
    "    else:\n",
    "        logging.error(f\"Unknown dataset_type: {dataset_type}\")\n",
    "\n",
    "    # Free Memory\n",
    "    if dataset_type == 'train_dev':\n",
    "        del train_df, val_df\n",
    "    elif dataset_type == 'test':\n",
    "        del test_df\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"{dataset_type} data for the {task_name} task have been saved in Pickle format.\")\n",
    "\n",
    "# List the Tasks to be Processed\n",
    "tasks = ['summary', 'qa', 'data2txt']\n",
    "\n",
    "# Map task_type for Each Task\n",
    "task_type_mapping = {\n",
    "    'summary': 'Summary',\n",
    "    'qa': 'QA',\n",
    "    'data2txt': 'Data2txt'\n",
    "}\n",
    "\n",
    "for task in tasks:\n",
    "    task_name = task.lower()\n",
    "    task_type = task_type_mapping.get(task_name, None)\n",
    "\n",
    "    if task_type is None:\n",
    "        logging.warning(f\"Unknown task_name: {task_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load and Preprocess Data\n",
    "    data_file = f'/home/code/data/saves/{model_type}_{task_name}.pkl'\n",
    "    if not os.path.exists(data_file):\n",
    "        logging.error(f\"Data file does not exist: {data_file}\")\n",
    "        continue\n",
    "\n",
    "    logging.info(f\"Loading data: {data_file}\")\n",
    "    try:\n",
    "        df = pd.read_pickle(data_file)\n",
    "        logging.info(\"Data loading completed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while loading data: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Check the first 'prompt' column\n",
    "    if 'prompt' not in df.columns:\n",
    "        logging.error(f\"The DataFrame does not contain a 'prompt' column. File: {data_file}\")\n",
    "        continue\n",
    "\n",
    "    # Create a label: 1 if at least one hallucination is present, 0 otherwise\n",
    "    df['label'] = df['labels'].apply(lambda x: 1 if len(x) > 0 else 0)\n",
    "\n",
    "    # Verify that data has been loaded\n",
    "    logging.info(f\"DataFrame columns: {df.columns}\")\n",
    "\n",
    "    # Filter based on the 'task_type' column\n",
    "    task_df = df[df['task_type'] == task_type].copy()\n",
    "    if task_df.empty:\n",
    "        logging.warning(f\"No data found for the {task_name} task.\")\n",
    "        continue\n",
    "\n",
    "    # Split into train_dev_df and test_df based on the 'split' column\n",
    "    logging.info(f\"Splitting {task_name} task data into train_dev and test...\")\n",
    "    train_dev_df = task_df[task_df['split'] == 'train'].copy()\n",
    "    test_df = task_df[task_df['split'] == 'test'].copy()\n",
    "    logging.info(f\"Number of train_dev samples: {train_dev_df.shape[0]}, Number of test samples: {test_df.shape[0]}\")\n",
    "\n",
    "    # Free memory\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"\\nStarting processing for the {task_name} task...\")\n",
    "\n",
    "    # Process train_dev data\n",
    "    logging.info(f\"Processing train_dev data for the {task_name} task...\")\n",
    "    main_processing(train_dev_df.copy(), task_name, 'train_dev')\n",
    "\n",
    "    # Free memory\n",
    "    del train_dev_df\n",
    "    gc.collect()\n",
    "\n",
    "    # Process test data\n",
    "    logging.info(f\"Processing test data for the {task_name} task...\")\n",
    "    main_processing(test_df.copy(), task_name, 'test')\n",
    "\n",
    "    # Free memory\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"{task_name} task data has been saved in Pickle format.\")\n",
    "\n",
    "    logging.info(\"\\nProcessing for all tasks has been completed.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
